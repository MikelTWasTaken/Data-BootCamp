{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ðŸŒŸ Exercise 1 : Parsing HTML with BeautifulSoup\n",
    "Instructions\n",
    "- Objective: Use urlopen() to fetch the HTML content of a webpage and then parse it using BeautifulSoup.\n",
    "\n",
    "- Read the HTML content of the page.\n",
    "- Create a BeautifulSoup object to parse this HTML.\n",
    "- Find the title of the webpage (the content inside the <title> tag).\n",
    "- Extract all paragraphs (<p> tags) from the page.\n",
    "- Retrieve all links (URLs in <a href=\"\"> tags) on the page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sports World'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = '''\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Sports World</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; }\n",
    "        header, nav, section, article, footer { margin: 20px; padding: 15px; }\n",
    "        nav { background-color: #333; }\n",
    "        nav a { color: white; padding: 14px 20px; text-decoration: none; display: inline-block; }\n",
    "        nav a:hover { background-color: #ddd; color: black; }\n",
    "        .video { text-align: center; margin: 20px 0; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <header>\n",
    "        <h1>Welcome to Sports World</h1>\n",
    "        <p>Your one-stop destination for the latest sports news and videos.</p>\n",
    "    </header>\n",
    "\n",
    "    <nav>\n",
    "        <a href=\"#football\">Football</a>\n",
    "        <a href=\"#basketball\">Basketball</a>\n",
    "        <a href=\"#tennis\">Tennis</a>\n",
    "    </nav>\n",
    "\n",
    "    <section id=\"football\">\n",
    "        <h2>Football</h2>\n",
    "        <article>\n",
    "            <h3>Latest Football News</h3>\n",
    "            <p>Read about the latest football matches and player news.</p>\n",
    "            <div class=\"video\">\n",
    "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/football-video-id\" frameborder=\"0\" allowfullscreen>\n",
    "                </iframe>\n",
    "            </div>\n",
    "        </article>\n",
    "    </section>\n",
    "\n",
    "    <section id=\"basketball\">\n",
    "        <h2>Basketball</h2>\n",
    "        <article>\n",
    "            <h3>NBA Highlights</h3>\n",
    "            <p>Watch highlights from the latest NBA games.</p>\n",
    "            <div class=\"video\">\n",
    "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/basketball-video-id\" frameborder=\"0\" allowfullscreen>\n",
    "                </iframe>\n",
    "            </div>\n",
    "        </article>\n",
    "    </section>\n",
    "\n",
    "    <section id=\"tennis\">\n",
    "        <h2>Tennis</h2>\n",
    "        <article>\n",
    "            <h3>Grand Slam Updates</h3>\n",
    "            <p>Get the latest updates from the world of Grand Slam tennis.</p>\n",
    "            <div class=\"video\">\n",
    "                <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tennis-video-id\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "            </div>\n",
    "        </article>\n",
    "    </section>\n",
    "\n",
    "    <footer>\n",
    "        <form action=\"mailto:contact@sportsworld.com\" method=\"post\" enctype=\"text/plain\">\n",
    "            <label for=\"name\">Name:</label><br>\n",
    "            <input type=\"text\" id=\"name\" name=\"name\"><br>\n",
    "            <label for=\"email\">Email:</label><br>\n",
    "            <input type=\"email\" id=\"email\" name=\"email\"><br>\n",
    "            <label for=\"message\">Message:</label><br>\n",
    "            <textarea id=\"message\" name=\"message\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Send\">\n",
    "        </form>\n",
    "    </footer>\n",
    "\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser') \n",
    "soup.html.head.title\n",
    "soup.title.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1: Your one-stop destination for the latest sports news and videos.\n",
      "Paragraph 2: Read about the latest football matches and player news.\n",
      "Paragraph 3: Watch highlights from the latest NBA games.\n",
      "Paragraph 4: Get the latest updates from the world of Grand Slam tennis.\n"
     ]
    }
   ],
   "source": [
    "# Extract all paragraphs (<p> tags) from the page.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Print each paragraph\n",
    "for index, paragraph in enumerate(paragraphs, start=1):\n",
    "    print(f\"Paragraph {index}: {paragraph.get_text(strip=True)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links (href attributes):\n",
      "Link 1: #football\n",
      "Link 2: #basketball\n",
      "Link 3: #tennis\n",
      "\n",
      "Tags with src attributes:\n",
      "Tag 1: https://www.youtube.com/embed/football-video-id\n",
      "Tag 2: https://www.youtube.com/embed/basketball-video-id\n",
      "Tag 3: https://www.youtube.com/embed/tennis-video-id\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all links (URLs in <a href=\"\"> tags) on the page.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print(\"Links (href attributes):\")\n",
    "for index, link in enumerate(links, start=1):\n",
    "    url = link.get('href')  # Get the href attribute\n",
    "    if url:\n",
    "        print(f\"Link {index}: {url}\")\n",
    "\n",
    "# Retrieve all tags with src attributes\n",
    "tags_with_src = soup.find_all(src=True)\n",
    "print(\"\\nTags with src attributes:\")\n",
    "for index, tag in enumerate(tags_with_src, start=1):\n",
    "    src_url = tag['src']  # Get the src attribute\n",
    "    print(f\"Tag {index}: {src_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 2 : Scraping robots.txt from Wikipedia:\n",
    "\n",
    "*Instructions*\n",
    "- Write a Python program to download and display the content of robot.txt for wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://en.wikipedia.org/robots.txt\")\n",
    "print(response.text[:100])\n",
    "websoup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 3 : Extracting Headers from Wikipediaâ€™s Main Page\n",
    "\n",
    "*Instructions*\n",
    "- Write a Python program to extract and display all the header tags from wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you knowÂ ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "soup1 = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "header_tags = soup1.find_all(['h1','h2','h3','h4','h5,','h6'])\n",
    "\n",
    "for header_tag in header_tags:\n",
    "    print(header_tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 4 : Checking for Page Title\n",
    "\n",
    "*Instructions*\n",
    "\n",
    "- Write a Python program to check whether a page contains a title or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia, the free encyclopedia\n"
     ]
    }
   ],
   "source": [
    "print(soup1.title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 5 : Analyzing US-CERT Security Alerts\n",
    "\n",
    "*Instructions*\n",
    "- Write a Python program to get the number of security alerts issued by US-CERT in the current year.\n",
    "Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.cisa.gov/news-events/cybersecurity-advisories?f%5B0%5D=advisory_type%3A93\"\n",
    "response = requests.get(url)\n",
    "Security = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of security alerts issued by US-CERT in the current year: 0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def get_security_alerts_count(url):\n",
    "    current_year = datetime.now().year\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return 0\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    alerts = soup.find_all('div', class_='c-teaser')\n",
    "    count = 0\n",
    "    for alert in alerts:\n",
    "        date_element = alert.find('year')\n",
    "        if date_element:\n",
    "            alert_date = date_element.get_text(strip=True)\n",
    "            alert_year = datetime.strptime(alert_date, '%B %d, %Y').year\n",
    "            if alert_year == current_year:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "url = \"https://www.cisa.gov/news-events/cybersecurity-advisories?f%5B0%5D=advisory_type%3A93\"\n",
    "alerts_count = get_security_alerts_count(url)\n",
    "print(f\"Number of security alerts issued by US-CERT in the current year: {alerts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 6 : Scraping Movie Details\n",
    "\n",
    "*Instructions*\n",
    "- Write a Python program to get movie name, year and a brief summary of the top 10 random movies from this IMBD website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "response = requests.get(url)\n",
    "hockey = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Teams:\n",
      "{'Team Name': 'Boston Bruins', 'Year': '1990', 'Wins': '44', 'Losses': '24', 'OT Losses': ''}\n",
      "{'Team Name': 'Buffalo Sabres', 'Year': '1990', 'Wins': '31', 'Losses': '30', 'OT Losses': ''}\n",
      "{'Team Name': 'Calgary Flames', 'Year': '1990', 'Wins': '46', 'Losses': '26', 'OT Losses': ''}\n",
      "{'Team Name': 'Chicago Blackhawks', 'Year': '1990', 'Wins': '49', 'Losses': '23', 'OT Losses': ''}\n",
      "{'Team Name': 'Detroit Red Wings', 'Year': '1990', 'Wins': '34', 'Losses': '38', 'OT Losses': ''}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_top_teams(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    if not table:\n",
    "        print(\"Could not find the team table.\")\n",
    "        return []\n",
    "\n",
    "    rows = table.find_all('tr')[1:] \n",
    "\n",
    "    top_teams = []\n",
    "    for row in rows[:5]:  \n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) < 5:\n",
    "            continue  \n",
    "        \n",
    "        team_name = cols[0].text.strip()\n",
    "        year = cols[1].text.strip()\n",
    "        wins = cols[2].text.strip()\n",
    "        losses = cols[3].text.strip()\n",
    "        ot_losses = cols[4].text.strip()\n",
    "\n",
    "        top_teams.append({\n",
    "            'Team Name': team_name,\n",
    "            'Year': year,\n",
    "            'Wins': wins,\n",
    "            'Losses': losses,\n",
    "            'OT Losses': ot_losses\n",
    "        })\n",
    "\n",
    "    return top_teams\n",
    "\n",
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "top_teams = scrape_top_teams(url)\n",
    "\n",
    "if top_teams:\n",
    "    print(\"Top 5 Teams:\")\n",
    "    for team in top_teams:\n",
    "        print(team)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
